{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BentoML Example\n",
    "# Titanic Survival Prediction with LightBGM\n",
    "\n",
    "\n",
    "BentoML is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps.**\n",
    "\n",
    "Data Scientists can easily package their models trained with any ML framework using BentoMl and reproduce the model for serving in production. BentoML helps with managing packaged models in the BentoML format, and allows DevOps to deploy them as online API serving endpoints or offline batch inference jobs, on any cloud platform.\n",
    "\n",
    "Before reading this example project, be sure to check out the [Getting started guide](https://github.com/bentoml/BentoML/blob/master/guides/quick-start/bentoml-quick-start-guide.ipynb) to learn about the basic concepts in BentoML.\n",
    "\n",
    "\n",
    "This notebook is demonstrating how to package and serve LightBGM model for production using BentoML.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=ligthbgm&ea=lightbgm-tiantic-survival-prediction&dt=lightbgm-tiantic-survival-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bentoml \"lightgbm==2.3.1\" numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bentoml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset\n",
    "download dataset from https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 60302  100 60302    0     0   154k      0 --:--:-- --:--:-- --:--:--  154k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 28210  100 28210    0     0   101k      0 --:--:-- --:--:-- --:--:--  101k\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv -o ./data/train.csv\n",
    "!curl https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/test.csv -o ./data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.pop('Survived')\n",
    "cols = ['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[cols], \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LGBM dataset for training\n",
    "train_data = lgb.Dataset(data=X_train[cols],\n",
    "                        label=y_train)\n",
    "\n",
    "# Create an LGBM dataset from the test\n",
    "test_data = lgb.Dataset(data=X_test[cols],\n",
    "                        label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tTrain's binary_logloss: 0.55215\tTest's binary_logloss: 0.587358\n",
      "[40]\tTrain's binary_logloss: 0.510164\tTest's binary_logloss: 0.559348\n",
      "[60]\tTrain's binary_logloss: 0.500602\tTest's binary_logloss: 0.551635\n",
      "[80]\tTrain's binary_logloss: 0.490215\tTest's binary_logloss: 0.547154\n",
      "[100]\tTrain's binary_logloss: 0.486812\tTest's binary_logloss: 0.547076\n",
      "[120]\tTrain's binary_logloss: 0.479242\tTest's binary_logloss: 0.542552\n",
      "[140]\tTrain's binary_logloss: 0.469847\tTest's binary_logloss: 0.539319\n",
      "[160]\tTrain's binary_logloss: 0.471384\tTest's binary_logloss: 0.542278\n",
      "[180]\tTrain's binary_logloss: 0.453052\tTest's binary_logloss: 0.535512\n",
      "[200]\tTrain's binary_logloss: 0.442048\tTest's binary_logloss: 0.533921\n",
      "[220]\tTrain's binary_logloss: 0.436788\tTest's binary_logloss: 0.534261\n",
      "[240]\tTrain's binary_logloss: 0.427196\tTest's binary_logloss: 0.532026\n",
      "[260]\tTrain's binary_logloss: 0.420145\tTest's binary_logloss: 0.531791\n",
      "[280]\tTrain's binary_logloss: 0.413336\tTest's binary_logloss: 0.527412\n",
      "[300]\tTrain's binary_logloss: 0.406546\tTest's binary_logloss: 0.529314\n",
      "[320]\tTrain's binary_logloss: 0.402753\tTest's binary_logloss: 0.525075\n",
      "[340]\tTrain's binary_logloss: 0.39979\tTest's binary_logloss: 0.523438\n",
      "[360]\tTrain's binary_logloss: 0.403024\tTest's binary_logloss: 0.525361\n",
      "[380]\tTrain's binary_logloss: 0.398387\tTest's binary_logloss: 0.528122\n",
      "[400]\tTrain's binary_logloss: 0.394841\tTest's binary_logloss: 0.529159\n",
      "[420]\tTrain's binary_logloss: 0.390478\tTest's binary_logloss: 0.528173\n",
      "[440]\tTrain's binary_logloss: 0.384254\tTest's binary_logloss: 0.526504\n",
      "[460]\tTrain's binary_logloss: 0.381594\tTest's binary_logloss: 0.525863\n",
      "[480]\tTrain's binary_logloss: 0.371362\tTest's binary_logloss: 0.527399\n",
      "[500]\tTrain's binary_logloss: 0.369978\tTest's binary_logloss: 0.525418\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'boosting': 'dart',          # dart (drop out trees) often performs better\n",
    "    'application': 'binary',     # Binary classification\n",
    "    'learning_rate': 0.05,       # Learning rate, controls size of a gradient descent step\n",
    "    'min_data_in_leaf': 20,      # Data set is quite small so reduce this a bit\n",
    "    'feature_fraction': 0.7,     # Proportion of features in each boost, controls overfitting\n",
    "    'num_leaves': 41,            # Controls size of tree since LGBM uses leaf wise splits\n",
    "    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric\n",
    "    'drop_rate': 0.15\n",
    "              }\n",
    "\n",
    "evaluation_results = {}\n",
    "model = lgb.train(train_set=train_data,\n",
    "                params=lgb_params,\n",
    "                valid_sets=[train_data, test_data], \n",
    "                valid_names=['Train', 'Test'],\n",
    "                evals_result=evaluation_results,\n",
    "                num_boost_round=500,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=20\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass   Age     Fare  SibSp  Parch      pred\n",
       "10       3   NaN   7.8958      0      0  0.052353\n",
       "11       1  46.0  26.0000      0      0  0.308877"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['pred'] = model.predict(test_df[cols])\n",
    "test_df[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch','pred']].iloc[10:].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BentoService for model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lightbgm_titanic_bento_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lightbgm_titanic_bento_service.py\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import bentoml\n",
    "from bentoml.frameworks.lightgbm import LightGBMModelArtifact\n",
    "from bentoml.adapters import DataframeInput\n",
    "\n",
    "@bentoml.artifacts([LightGBMModelArtifact('model')])\n",
    "@bentoml.env(pip_packages=['lightgbm'])\n",
    "class TitanicSurvivalPredictionService(bentoml.BentoService):\n",
    "    \n",
    "    @bentoml.api(input=DataframeInput())\n",
    "    def predict(self, df):\n",
    "        data = df[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']]\n",
    "        return self.artifacts.model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save BentoML service archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-04 10:02:12,116] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-08-04 10:02:22,865] INFO - Detect BentoML installed in development model, copying local BentoML module file to target saved bundle path\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "warning: no directories found matching 'bentoml/server/static'\n",
      "warning: no directories found matching 'bentoml/yatai/web/dist'\n",
      "no previously-included directories found matching 'e2e_tests'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'benchmark'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating BentoML-0.8.3+47.g5daa71b\n",
      "creating BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/clipper\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/configuration\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/handlers\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/marshal\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/client\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/aws_lambda\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/__pycache__\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/versions\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/versions/__pycache__\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "creating BentoML-0.8.3+47.g5daa71b/bentoml/yatai/validator\n",
      "copying files to BentoML-0.8.3+47.g5daa71b...\n",
      "copying LICENSE -> BentoML-0.8.3+47.g5daa71b\n",
      "copying MANIFEST.in -> BentoML-0.8.3+47.g5daa71b\n",
      "copying README.md -> BentoML-0.8.3+47.g5daa71b\n",
      "copying pyproject.toml -> BentoML-0.8.3+47.g5daa71b\n",
      "copying setup.cfg -> BentoML-0.8.3+47.g5daa71b\n",
      "copying setup.py -> BentoML-0.8.3+47.g5daa71b\n",
      "copying versioneer.py -> BentoML-0.8.3+47.g5daa71b\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.8.3+47.g5daa71b/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.8.3+47.g5daa71b/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.8.3+47.g5daa71b/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.8.3+47.g5daa71b/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.8.3+47.g5daa71b/bentoml\n",
      "copying bentoml/adapters/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/base_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/base_output.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/clipper_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/dataframe_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/dataframe_output.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/default_output.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/fastai_image_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/file_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/image_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/json_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/json_output.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/legacy_image_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/legacy_json_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/multi_image_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/pytorch_tensor_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/tensorflow_tensor_input.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/tensorflow_tensor_output.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/adapters/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/adapters\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/fastai2_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/fasttext_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/json_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/onnx_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/spacy_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.8.3+47.g5daa71b/bentoml/artifact\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/aws_lambda.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/aws_sagemaker.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/azure_functions.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/bento_management.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/bento_service.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/cli/yatai_service.py -> BentoML-0.8.3+47.g5daa71b/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-36.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-37.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-38.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-36.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-37.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-38.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/configuration/__pycache__\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/handlers\n",
      "copying bentoml/marshal/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/marshal\n",
      "copying bentoml/marshal/dispatcher.py -> BentoML-0.8.3+47.g5daa71b/bentoml/marshal\n",
      "copying bentoml/marshal/marshal.py -> BentoML-0.8.3+47.g5daa71b/bentoml/marshal\n",
      "copying bentoml/marshal/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/marshal\n",
      "copying bentoml/saved_bundle/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/bentoml-init.sh -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/bundler.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/config.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/docker-entrypoint.sh -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/loader.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/pip_pkg.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/py_module_utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/saved_bundle/templates.py -> BentoML-0.8.3+47.g5daa71b/bentoml/saved_bundle\n",
      "copying bentoml/server/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/api_server.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/instruments.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/marshal_server.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/open_api.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/trace.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/server\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/alg.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/benchmark.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/dataframe_util.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/flask_ngrok.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/hybridmethod.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/lazy_loader.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.8.3+47.g5daa71b/bentoml/utils\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/alembic.ini -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/db.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai\n",
      "copying bentoml/yatai/client/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/bento_repository_api.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/deployment_api.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/client\n",
      "copying bentoml/yatai/deployment/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment\n",
      "copying bentoml/yatai/deployment/operator.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment\n",
      "copying bentoml/yatai/deployment/store.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment\n",
      "copying bentoml/yatai/deployment/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment\n",
      "copying bentoml/yatai/deployment/aws_lambda/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/aws_lambda\n",
      "copying bentoml/yatai/deployment/aws_lambda/download_extra_resources.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/aws_lambda\n",
      "copying bentoml/yatai/deployment/aws_lambda/lambda_app.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/aws_lambda\n",
      "copying bentoml/yatai/deployment/aws_lambda/operator.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/aws_lambda\n",
      "copying bentoml/yatai/deployment/aws_lambda/utils.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/aws_lambda\n",
      "copying bentoml/yatai/deployment/azure_functions/Dockerfile -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/app_init.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/constants.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/host.json -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/local.settings.json -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/operator.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/azure_functions/templates.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/azure_functions\n",
      "copying bentoml/yatai/deployment/sagemaker/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "copying bentoml/yatai/deployment/sagemaker/model_server.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "copying bentoml/yatai/deployment/sagemaker/nginx.conf -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "copying bentoml/yatai/deployment/sagemaker/operator.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "copying bentoml/yatai/deployment/sagemaker/serve -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "copying bentoml/yatai/deployment/sagemaker/wsgi.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/deployment/sagemaker\n",
      "copying bentoml/yatai/migrations/README -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations\n",
      "copying bentoml/yatai/migrations/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations\n",
      "copying bentoml/yatai/migrations/env.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations\n",
      "copying bentoml/yatai/migrations/script.py.mako -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations\n",
      "copying bentoml/yatai/migrations/__pycache__/env.cpython-36.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/__pycache__\n",
      "copying bentoml/yatai/migrations/versions/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/versions\n",
      "copying bentoml/yatai/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/versions\n",
      "copying bentoml/yatai/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-36.pyc -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/migrations/versions/__pycache__\n",
      "copying bentoml/yatai/proto/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "copying bentoml/yatai/proto/deployment_pb2.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "copying bentoml/yatai/proto/repository_pb2.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "copying bentoml/yatai/proto/status_pb2.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "copying bentoml/yatai/proto/yatai_service_pb2.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "copying bentoml/yatai/proto/yatai_service_pb2_grpc.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/proto\n",
      "copying bentoml/yatai/repository/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "copying bentoml/yatai/repository/base_repository.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "copying bentoml/yatai/repository/local_repository.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "copying bentoml/yatai/repository/metadata_store.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "copying bentoml/yatai/repository/repository.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "copying bentoml/yatai/repository/s3_repository.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/repository\n",
      "copying bentoml/yatai/validator/__init__.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/validator\n",
      "copying bentoml/yatai/validator/deployment_pb_validator.py -> BentoML-0.8.3+47.g5daa71b/bentoml/yatai/validator\n",
      "Writing BentoML-0.8.3+47.g5daa71b/setup.cfg\n",
      "UPDATING BentoML-0.8.3+47.g5daa71b/bentoml/_version.py\n",
      "set BentoML-0.8.3+47.g5daa71b/bentoml/_version.py to '0.8.3+47.g5daa71b'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.8.3+47.g5daa71b' (and everything under it)\n",
      "[2020-08-04 10:02:23,797] INFO - BentoService bundle 'TitanicSurvivalPredictionService:20200804100212_2598E6' saved to: /home/bentoml/bentoml/repository/TitanicSurvivalPredictionService/20200804100212_2598E6\n"
     ]
    }
   ],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from lightbgm_titanic_bento_service import TitanicSurvivalPredictionService\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "bento_service = TitanicSurvivalPredictionService()\n",
    "bento_service.pack('model', model)\n",
    "\n",
    "# 3) save your BentoSerivce\n",
    "saved_path = bento_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "\n",
    "To start a REST API model server with the BentoService saved above, use the bentoml serve command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-04 10:05:18,992] INFO - Getting latest version TitanicSurvivalPredictionService:20200804100212_2598E6\n",
      "[2020-08-04 10:05:18,992] INFO - Starting BentoML API server in development mode..\n",
      "[2020-08-04 10:05:19,966] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-08-04 10:05:19,988] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[2020-08-04 10:05:21,477] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[2020-08-04 10:05:21,478] INFO - Micro batch enabled for API `predict`\n",
      "[2020-08-04 10:05:21,480] INFO - Your system nofile limit is 10000, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2020-08-04 10:05:21,488] INFO - Running micro batch service on :5000\n",
      " * Serving Flask app \"TitanicSurvivalPredictionService\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:52753/ (Press CTRL+C to quit)\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      "127.0.0.1 - - [04/Aug/2020 10:05:33] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [04/Aug/2020 10:05:35] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [04/Aug/2020 10:05:36] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve TitanicSurvivalPredictionService:latest --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook from Google Colab, you can start the dev server with `--run-with-ngrok` option, to gain acccess to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open http://127.0.0.1:5000 to see more information about the REST APIs server in your\n",
    "browser.\n",
    "\n",
    "\n",
    "### Send prediction requeset to the REST API server\n",
    "\n",
    "Navigate to parent directory of the notebook(so you have reference to the `test.jpg` image), and run the following `curl` command to send the image to REST API server and get a prediction result:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[{\"Pclass\": 1, \"Age\": 30, \"Fare\": 200, \"SibSp\": 1, \"Parch\": 0}]' \\\n",
    "localhost:5000/predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containerize model server with Docker\n",
    "\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that docker is **not available in Google Colab**. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a docker container serving the IrisClassifier prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:cc5736d088e4beea88863682c5107dd4d2a8e067670db1cf667134569936896f\n"
     ]
    }
   ],
   "source": [
    "!bentoml containerize TitanicSurvivalPredictionService:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-04 02:09:42,448] INFO - Starting BentoML API server in production mode..\n",
      "[2020-08-04 02:09:42,880] INFO - get_gunicorn_num_of_workers: 3, calculated by cpu count\n",
      "[2020-08-04 02:09:42,890] INFO - Running micro batch service on :5000\n",
      "[2020-08-04 02:09:42 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-08-04 02:09:42 +0000] [12] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-08-04 02:09:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:40927 (1)\n",
      "[2020-08-04 02:09:42 +0000] [12] [INFO] Listening at: http://0.0.0.0:5000 (12)\n",
      "[2020-08-04 02:09:42 +0000] [12] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2020-08-04 02:09:42 +0000] [1] [INFO] Using worker: sync\n",
      "[2020-08-04 02:09:42 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "[2020-08-04 02:09:42 +0000] [14] [INFO] Booting worker with pid: 14\n",
      "[2020-08-04 02:09:42 +0000] [15] [INFO] Booting worker with pid: 15\n",
      "[2020-08-04 02:09:42,920] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deplying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-08-04 02:09:42,940] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[2020-08-04 02:09:43 +0000] [16] [INFO] Booting worker with pid: 16\n",
      "[2020-08-04 02:09:43,187] INFO - Micro batch enabled for API `predict`\n",
      "[2020-08-04 02:09:43,187] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2020-08-04 02:09:43,775] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deplying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-08-04 02:09:43,791] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deplying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-08-04 02:09:43,800] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deplying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-08-04 02:09:43,825] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[2020-08-04 02:09:43,831] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[2020-08-04 02:09:43,836] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "^C\n",
      "[2020-08-04 02:09:50 +0000] [1] [INFO] Handling signal: int\n",
      "[2020-08-04 02:09:51 +0000] [14] [INFO] Worker exiting (pid: 14)\n",
      "[2020-08-04 02:09:51 +0000] [15] [INFO] Worker exiting (pid: 15)\n",
      "[2020-08-04 02:09:51 +0000] [16] [INFO] Worker exiting (pid: 16)\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -p 5000:5000 TitanicSurvivalPredictionService --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved BentoService for serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-04 10:02:31,584] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[2020-08-04 10:02:31,585] WARNING - Module `lightbgm_titanic_bento_service` already loaded, using existing imported module.\n",
      "[2020-08-04 10:02:31,587] WARNING - `load` on a 'packed' artifact may lead to unexpected behaviors\n",
      "[2020-08-04 10:02:31,622] WARNING - `pack` an artifact multiple times may lead to unexpected behaviors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.308877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass   Age     Fare  SibSp  Parch      pred\n",
       "10       3   NaN   7.8958      0      0  0.052353\n",
       "11       1  46.0  26.0000      0      0  0.308877"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "bento_model = bentoml.load(saved_path)\n",
    "\n",
    "result = bento_model.predict(test_df)\n",
    "test_df['pred'] = result\n",
    "test_df[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch','pred']].iloc[10:].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch inference job from CLI\n",
    "\n",
    "BentoML cli supports loading and running a packaged model from CLI. With the DataframeInput adapter, the CLI command supports reading input Dataframe data from CLI argument or local csv or json files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-04 10:04:51,776] INFO - Getting latest version TitanicSurvivalPredictionService:20200804100212_2598E6\n",
      "[2020-08-04 10:04:52,809] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-08-04 10:04:52,836] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.3, but loading from BentoML version 0.8.3+47.g5daa71b\n",
      "[0.50459633]\n"
     ]
    }
   ],
   "source": [
    "!bentoml run TitanicSurvivalPredictionService:latest predict \\\n",
    "--input '{\"PassengerId\":{\"3\":895},\"Pclass\":{\"3\":3},\"Name\":{\"3\":\"Wirz, Mr. Albert\"},\"Sex\":{\"3\":\"male\"},\"Age\":{\"3\":27.0},\"SibSp\":{\"3\":0},\"Parch\":{\"3\":0},\"Ticket\":{\"3\":\"315154\"},\"Fare\":{\"3\":8.6625},\"Cabin\":{\"3\":null},\"Embarked\":{\"3\":\"S\"},\"pred\":{\"3\":0.5045963287}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Options\n",
    "\n",
    "If you are at a small team with limited engineering or DevOps resources, try out automated deployment with BentoML CLI, currently supporting AWS Lambda, AWS SageMaker, and Azure Functions:\n",
    "- [AWS Lambda Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
    "- [AWS SageMaker Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
    "- [Azure Functions Deployment Guide](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
    "\n",
    "If the cloud platform you are working with is not on the list above, try out these step-by-step guide on manually deploying BentoML packaged model to cloud platforms:\n",
    "- [AWS ECS Deployment](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
    "- [Google Cloud Run Deployment](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
    "- [Azure container instance Deployment](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
    "- [Heroku Deployment](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
    "\n",
    "Lastly, if you have a DevOps or ML Engineering team who's operating a Kubernetes or OpenShift cluster, use the following guides as references for implementating your deployment strategy:\n",
    "- [Kubernetes Deployment](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
    "- [Knative Deployment](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
    "- [Kubeflow Deployment](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
    "- [KFServing Deployment](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
    "- [Clipper.ai Deployment Guide](https://docs.bentoml.org/en/latest/deployment/clipper.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
