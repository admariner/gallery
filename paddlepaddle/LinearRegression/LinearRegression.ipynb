{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erfOlc-T8kY3"
   },
   "source": [
    "# **BentoML Example: Linear Regression with Paddlepaddle**\n",
    "**BentoML makes moving trained ML models to production easy:**\n",
    "\n",
    "\n",
    "\n",
    "*   Package models trained with any ML framework and reproduce them for model serving in production\n",
    "* **Deploy anywhere** for online API serving or offline batch serving\n",
    "* High-Performance API model server with adaptive micro-batching support\n",
    "* Central hub for managing models and deployment process via Web UI and APIs\n",
    "* Modular and flexible design making it adaptable to your infrastrcuture\n",
    "\n",
    "BentoML is a framework for serving, managing, and deploying machine learning models. It is aiming to bridge the gap between Data Science and DevOps, and enable teams to deliver prediction services in a fast, repeatable, and scalable way.\n",
    "\n",
    "Before reading this example project, be sure to check out the [Getting started guide](https://github.com/bentoml/BentoML/blob/master/guides/quick-start/bentoml-quick-start-guide.ipynb) to learn about the basic concepts in BentoML.\n",
    "\n",
    "This notebook demonstrates how to use BentoML to turn a paddlepaddle model into a docker image containing a REST API server serving this model, how to use your ML service built with BentoML as a CLI tool, and how to distribute it a pypi package.\n",
    "\n",
    "The example is based on [this tutorial](https://www.paddlepaddle.org.cn/documentation/docs/en/1.5/beginners_guide/basics/fit_a_line/README.html), using dataset from the [UCI Machine Learning Repository](https://www.kaggle.com/schirmerchad/bostonhoustingmlnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "54jFhiru8NWO"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHOPuMGm-Nl2",
    "outputId": "5e08ea9b-b356-4908-a3a2-b949b733f4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/bentoml/BentoML.git\n",
      "  Cloning https://github.com/bentoml/BentoML.git to /private/var/folders/kn/xnc9k74x03567n1mx2tfqnpr0000gn/T/pip-req-build-y5l3v5a6\n",
      "  Running command git clone -q https://github.com/bentoml/BentoML.git /private/var/folders/kn/xnc9k74x03567n1mx2tfqnpr0000gn/T/pip-req-build-y5l3v5a6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: humanfriendly in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (9.1)\n",
      "Requirement already satisfied: urllib3<=1.25.11 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.25.11)\n",
      "Requirement already satisfied: gunicorn in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (20.0.4)\n",
      "Requirement already satisfied: configparser in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (5.0.2)\n",
      "Requirement already satisfied: sqlalchemy<1.4.0,>=1.3.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.3.23)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (2.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.20.1)\n",
      "Requirement already satisfied: docker in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (4.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (2.8.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.15.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (0.16.13)\n",
      "Requirement already satisfied: dependency-injector<5.0,>=4.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (4.29.2)\n",
      "Requirement already satisfied: tabulate in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (0.8.9)\n",
      "Requirement already satisfied: flask in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.0.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (3.15.6)\n",
      "Requirement already satisfied: chardet in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (3.0.4)\n",
      "Requirement already satisfied: cerberus in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.3.2)\n",
      "Requirement already satisfied: certifi in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (2020.12.5)\n",
      "Requirement already satisfied: packaging in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (20.9)\n",
      "Requirement already satisfied: alembic in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.5.7)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (7.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.17.30)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (0.9.0)\n",
      "Requirement already satisfied: sqlalchemy-utils<0.36.8 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (0.36.7)\n",
      "Requirement already satisfied: deepmerge in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (0.2.1)\n",
      "Requirement already satisfied: grpcio in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (1.36.1)\n",
      "Requirement already satisfied: schema in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (0.7.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (3.7.4.post0)\n",
      "Requirement already satisfied: psutil in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from BentoML==0.12.0+15.g7d40998) (5.8.0)\n",
      "Requirement already satisfied: six<=1.15.0,>=1.7.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from dependency-injector<5.0,>=4.0->BentoML==0.12.0+15.g7d40998) (1.15.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from ruamel.yaml>=0.15.0->BentoML==0.12.0+15.g7d40998) (0.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from aiohttp->BentoML==0.12.0+15.g7d40998) (20.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from aiohttp->BentoML==0.12.0+15.g7d40998) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from aiohttp->BentoML==0.12.0+15.g7d40998) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from aiohttp->BentoML==0.12.0+15.g7d40998) (3.7.4.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from aiohttp->BentoML==0.12.0+15.g7d40998) (3.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->BentoML==0.12.0+15.g7d40998) (2.8)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from alembic->BentoML==0.12.0+15.g7d40998) (1.0.4)\n",
      "Requirement already satisfied: Mako in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from alembic->BentoML==0.12.0+15.g7d40998) (1.1.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from boto3->BentoML==0.12.0+15.g7d40998) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.30 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from boto3->BentoML==0.12.0+15.g7d40998) (1.20.30)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from boto3->BentoML==0.12.0+15.g7d40998) (0.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from cerberus->BentoML==0.12.0+15.g7d40998) (49.6.0.post20210108)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from docker->BentoML==0.12.0+15.g7d40998) (0.58.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from flask->BentoML==0.12.0+15.g7d40998) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from flask->BentoML==0.12.0+15.g7d40998) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from flask->BentoML==0.12.0+15.g7d40998) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from Jinja2>=2.10->flask->BentoML==0.12.0+15.g7d40998) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from packaging->BentoML==0.12.0+15.g7d40998) (2.4.7)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from schema->BentoML==0.12.0+15.g7d40998) (0.6.0.post1)\n",
      "Building wheels for collected packages: BentoML\n",
      "  Building wheel for BentoML (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BentoML: filename=BentoML-0.12.0+15.g7d40998-py3-none-any.whl size=674023 sha256=5bd65f7f3eadd7e78aad7142a77ecf2619efe6b6147f1fc8fb0fb7be343a17d5\n",
      "  Stored in directory: /private/var/folders/kn/xnc9k74x03567n1mx2tfqnpr0000gn/T/pip-ephem-wheel-cache-taoztxg4/wheels/dd/93/68/1a11f763a858794f96abf3c1def8b101125e236de566ebeb25\n",
      "Successfully built BentoML\n",
      "Installing collected packages: BentoML\n",
      "  Attempting uninstall: BentoML\n",
      "    Found existing installation: BentoML 0.12.0+14.gb6fe25cf\n",
      "    Uninstalling BentoML-0.12.0+14.gb6fe25cf:\n",
      "      Successfully uninstalled BentoML-0.12.0+14.gb6fe25cf\n",
      "Successfully installed BentoML-0.12.0+15.g7d40998\n",
      "Collecting paddlepaddle\n",
      "  Downloading paddlepaddle-2.0.1-cp37-cp37m-macosx_10_6_intel.whl (74.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 74.9 MB 99 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from paddlepaddle) (1.20.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from paddlepaddle) (8.1.2)\n",
      "Collecting gast>=0.3.3\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astor\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: six in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from paddlepaddle) (1.15.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from paddlepaddle) (2.22.0)\n",
      "Requirement already satisfied: decorator in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from paddlepaddle) (5.0.3)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from paddlepaddle) (3.15.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (1.25.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle) (3.0.4)\n",
      "Installing collected packages: gast, astor, paddlepaddle\n",
      "Successfully installed astor-0.8.1 gast-0.4.0 paddlepaddle-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bentoml\n",
    "\n",
    "#!pip install -q bentoml 'paddlepaddle>=2.0.0' 'pandas>=1.1.1' 'numpy>=1.8.2'\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTmhsmvk-iZT",
    "outputId": "0476512f-ff91-4866-edbc-cfaf53aab51e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as opt\n",
    "import pandas as pd\n",
    "\n",
    "import bentoml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CeOjFaR_e2x"
   },
   "source": [
    "# **Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yayroXhE-sos",
    "outputId": "3583d945-5047-47e2-8756-6b4c5b3f5af0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import bentoml\n",
    "from paddle.static import InputSpec\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "BATCH_NUM = 4\n",
    "EPOCH_NUM = 5\n",
    "\n",
    "IN_FEATURES = 13\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "class LinearNet(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self._linear = paddle.nn.Linear(IN_FEATURES, OUT_FEATURES)\n",
    "\n",
    "    @paddle.jit.to_static(input_spec=[InputSpec(shape=[IN_FEATURES], dtype='float32')])\n",
    "    def forward(self, x):\n",
    "        return self._linear(x)\n",
    "\n",
    "    def train(self, loader, loss_fn, opt):\n",
    "        for epoch_id in range(EPOCH_NUM):\n",
    "            for batch_id, (image, label) in enumerate(loader()):\n",
    "                out = self._linear(image)\n",
    "                loss = loss_fn(out, label)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.clear_grad()\n",
    "                print(\"Epoch {} batch {}: loss = {}\".format(\n",
    "                    epoch_id, batch_id, np.mean(loss.numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEiqsFpJ_qk-"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hhiqk3VP_mqe",
    "outputId": "04ca085e-629f-4f6d-f82c-4f06ef1f580f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /Users/bozhaoyu/.cache/paddle/dataset/uci_housing/housing.data not found, downloading http://paddlemodels.bj.bcebos.com/uci_housing/housing.data \n",
      "Begin to download\n",
      "............\n",
      "Download finished\n",
      "/usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages/paddle/fluid/reader.py:352: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead\n",
      "  \"DataLoader with multi-process mode is not supported on MacOs and Windows currently.\" \\\n",
      "/usr/local/Caskroom/miniconda/base/envs/dev/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 0: loss = 435.19635009765625\n",
      "Epoch 0 batch 1: loss = 421.4579772949219\n",
      "Epoch 0 batch 2: loss = 459.74871826171875\n",
      "Epoch 0 batch 3: loss = 680.4847412109375\n",
      "Epoch 0 batch 4: loss = 858.70458984375\n",
      "Epoch 0 batch 5: loss = 536.1829833984375\n",
      "Epoch 0 batch 6: loss = 555.8958740234375\n",
      "Epoch 0 batch 7: loss = 388.1265563964844\n",
      "Epoch 0 batch 8: loss = 647.7759399414062\n",
      "Epoch 0 batch 9: loss = 494.9188232421875\n",
      "Epoch 0 batch 10: loss = 970.626220703125\n",
      "Epoch 0 batch 11: loss = 659.16064453125\n",
      "Epoch 0 batch 12: loss = 983.6190185546875\n",
      "Epoch 0 batch 13: loss = 936.4552612304688\n",
      "Epoch 0 batch 14: loss = 1019.9678344726562\n",
      "Epoch 0 batch 15: loss = 747.4344482421875\n",
      "Epoch 0 batch 16: loss = 831.9373168945312\n",
      "Epoch 0 batch 17: loss = 350.56976318359375\n",
      "Epoch 0 batch 18: loss = 515.817626953125\n",
      "Epoch 0 batch 19: loss = 713.2019653320312\n",
      "Epoch 0 batch 20: loss = 610.8594360351562\n",
      "Epoch 0 batch 21: loss = 594.1142578125\n",
      "Epoch 0 batch 22: loss = 471.9237060546875\n",
      "Epoch 0 batch 23: loss = 1203.9696044921875\n",
      "Epoch 0 batch 24: loss = 600.8118286132812\n",
      "Epoch 0 batch 25: loss = 372.2288818359375\n",
      "Epoch 0 batch 26: loss = 596.131103515625\n",
      "Epoch 0 batch 27: loss = 712.577392578125\n",
      "Epoch 0 batch 28: loss = 823.0017700195312\n",
      "Epoch 0 batch 29: loss = 838.0972290039062\n",
      "Epoch 0 batch 30: loss = 755.33935546875\n",
      "Epoch 0 batch 31: loss = 414.0630798339844\n",
      "Epoch 0 batch 32: loss = 482.1142883300781\n",
      "Epoch 0 batch 33: loss = 559.952880859375\n",
      "Epoch 0 batch 34: loss = 896.4400634765625\n",
      "Epoch 0 batch 35: loss = 1178.97607421875\n",
      "Epoch 0 batch 36: loss = 344.52386474609375\n",
      "Epoch 0 batch 37: loss = 696.4715576171875\n",
      "Epoch 0 batch 38: loss = 587.26611328125\n",
      "Epoch 0 batch 39: loss = 524.2654418945312\n",
      "Epoch 0 batch 40: loss = 719.9611206054688\n",
      "Epoch 0 batch 41: loss = 883.761962890625\n",
      "Epoch 0 batch 42: loss = 605.924560546875\n",
      "Epoch 0 batch 43: loss = 790.5938720703125\n",
      "Epoch 0 batch 44: loss = 860.56396484375\n",
      "Epoch 0 batch 45: loss = 1057.826904296875\n",
      "Epoch 0 batch 46: loss = 558.65869140625\n",
      "Epoch 0 batch 47: loss = 657.4581909179688\n",
      "Epoch 0 batch 48: loss = 440.3964538574219\n",
      "Epoch 0 batch 49: loss = 741.140625\n",
      "Epoch 1 batch 0: loss = 503.2971496582031\n",
      "Epoch 1 batch 1: loss = 574.934326171875\n",
      "Epoch 1 batch 2: loss = 791.82470703125\n",
      "Epoch 1 batch 3: loss = 561.6544189453125\n",
      "Epoch 1 batch 4: loss = 576.545166015625\n",
      "Epoch 1 batch 5: loss = 824.7929077148438\n",
      "Epoch 1 batch 6: loss = 514.4303588867188\n",
      "Epoch 1 batch 7: loss = 480.4468688964844\n",
      "Epoch 1 batch 8: loss = 910.288818359375\n",
      "Epoch 1 batch 9: loss = 428.64263916015625\n",
      "Epoch 1 batch 10: loss = 812.5103759765625\n",
      "Epoch 1 batch 11: loss = 1199.470703125\n",
      "Epoch 1 batch 12: loss = 811.8062744140625\n",
      "Epoch 1 batch 13: loss = 474.69183349609375\n",
      "Epoch 1 batch 14: loss = 431.4730224609375\n",
      "Epoch 1 batch 15: loss = 681.200927734375\n",
      "Epoch 1 batch 16: loss = 543.3580322265625\n",
      "Epoch 1 batch 17: loss = 643.1007080078125\n",
      "Epoch 1 batch 18: loss = 621.5284423828125\n",
      "Epoch 1 batch 19: loss = 760.8117065429688\n",
      "Epoch 1 batch 20: loss = 499.61346435546875\n",
      "Epoch 1 batch 21: loss = 910.94970703125\n",
      "Epoch 1 batch 22: loss = 549.989990234375\n",
      "Epoch 1 batch 23: loss = 555.82763671875\n",
      "Epoch 1 batch 24: loss = 453.82476806640625\n",
      "Epoch 1 batch 25: loss = 445.43994140625\n",
      "Epoch 1 batch 26: loss = 722.4783325195312\n",
      "Epoch 1 batch 27: loss = 770.901611328125\n",
      "Epoch 1 batch 28: loss = 782.9249267578125\n",
      "Epoch 1 batch 29: loss = 635.4414672851562\n",
      "Epoch 1 batch 30: loss = 806.8521118164062\n",
      "Epoch 1 batch 31: loss = 575.8161010742188\n",
      "Epoch 1 batch 32: loss = 601.47265625\n",
      "Epoch 1 batch 33: loss = 853.6051025390625\n",
      "Epoch 1 batch 34: loss = 943.209716796875\n",
      "Epoch 1 batch 35: loss = 755.8599853515625\n",
      "Epoch 1 batch 36: loss = 850.079833984375\n",
      "Epoch 1 batch 37: loss = 812.576416015625\n",
      "Epoch 1 batch 38: loss = 578.6965942382812\n",
      "Epoch 1 batch 39: loss = 755.2635498046875\n",
      "Epoch 1 batch 40: loss = 615.0452880859375\n",
      "Epoch 1 batch 41: loss = 514.7759399414062\n",
      "Epoch 1 batch 42: loss = 718.6151733398438\n",
      "Epoch 1 batch 43: loss = 710.81787109375\n",
      "Epoch 1 batch 44: loss = 1070.00439453125\n",
      "Epoch 1 batch 45: loss = 599.828125\n",
      "Epoch 1 batch 46: loss = 420.0142822265625\n",
      "Epoch 1 batch 47: loss = 317.08905029296875\n",
      "Epoch 1 batch 48: loss = 772.1884765625\n",
      "Epoch 1 batch 49: loss = 581.118896484375\n",
      "Epoch 2 batch 0: loss = 862.5804443359375\n",
      "Epoch 2 batch 1: loss = 462.8978271484375\n",
      "Epoch 2 batch 2: loss = 652.2523803710938\n",
      "Epoch 2 batch 3: loss = 1217.7396240234375\n",
      "Epoch 2 batch 4: loss = 628.7020874023438\n",
      "Epoch 2 batch 5: loss = 531.6495361328125\n",
      "Epoch 2 batch 6: loss = 330.69866943359375\n",
      "Epoch 2 batch 7: loss = 770.7513427734375\n",
      "Epoch 2 batch 8: loss = 401.77020263671875\n",
      "Epoch 2 batch 9: loss = 863.222412109375\n",
      "Epoch 2 batch 10: loss = 533.8890380859375\n",
      "Epoch 2 batch 11: loss = 506.18646240234375\n",
      "Epoch 2 batch 12: loss = 888.6613159179688\n",
      "Epoch 2 batch 13: loss = 543.69384765625\n",
      "Epoch 2 batch 14: loss = 535.5054321289062\n",
      "Epoch 2 batch 15: loss = 403.19146728515625\n",
      "Epoch 2 batch 16: loss = 489.041259765625\n",
      "Epoch 2 batch 17: loss = 581.9658813476562\n",
      "Epoch 2 batch 18: loss = 388.47894287109375\n",
      "Epoch 2 batch 19: loss = 1026.50341796875\n",
      "Epoch 2 batch 20: loss = 588.2946166992188\n",
      "Epoch 2 batch 21: loss = 901.6898193359375\n",
      "Epoch 2 batch 22: loss = 576.9415283203125\n",
      "Epoch 2 batch 23: loss = 816.2774658203125\n",
      "Epoch 2 batch 24: loss = 671.8216552734375\n",
      "Epoch 2 batch 25: loss = 806.2049560546875\n",
      "Epoch 2 batch 26: loss = 747.4718627929688\n",
      "Epoch 2 batch 27: loss = 565.2826538085938\n",
      "Epoch 2 batch 28: loss = 653.98193359375\n",
      "Epoch 2 batch 29: loss = 611.644775390625\n",
      "Epoch 2 batch 30: loss = 852.8026123046875\n",
      "Epoch 2 batch 31: loss = 428.0990295410156\n",
      "Epoch 2 batch 32: loss = 941.274658203125\n",
      "Epoch 2 batch 33: loss = 452.7620849609375\n",
      "Epoch 2 batch 34: loss = 897.3431396484375\n",
      "Epoch 2 batch 35: loss = 737.9261474609375\n",
      "Epoch 2 batch 36: loss = 919.672607421875\n",
      "Epoch 2 batch 37: loss = 688.052001953125\n",
      "Epoch 2 batch 38: loss = 483.1783142089844\n",
      "Epoch 2 batch 39: loss = 623.8746337890625\n",
      "Epoch 2 batch 40: loss = 789.4937744140625\n",
      "Epoch 2 batch 41: loss = 497.65966796875\n",
      "Epoch 2 batch 42: loss = 529.1936645507812\n",
      "Epoch 2 batch 43: loss = 874.4013671875\n",
      "Epoch 2 batch 44: loss = 884.49658203125\n",
      "Epoch 2 batch 45: loss = 842.5672607421875\n",
      "Epoch 2 batch 46: loss = 396.98028564453125\n",
      "Epoch 2 batch 47: loss = 460.8985595703125\n",
      "Epoch 2 batch 48: loss = 816.37158203125\n",
      "Epoch 2 batch 49: loss = 712.0831909179688\n",
      "Epoch 3 batch 0: loss = 649.838134765625\n",
      "Epoch 3 batch 1: loss = 739.3240356445312\n",
      "Epoch 3 batch 2: loss = 652.0592041015625\n",
      "Epoch 3 batch 3: loss = 595.1077880859375\n",
      "Epoch 3 batch 4: loss = 672.396484375\n",
      "Epoch 3 batch 5: loss = 769.217041015625\n",
      "Epoch 3 batch 6: loss = 376.50384521484375\n",
      "Epoch 3 batch 7: loss = 565.156005859375\n",
      "Epoch 3 batch 8: loss = 555.4974975585938\n",
      "Epoch 3 batch 9: loss = 1074.7545166015625\n",
      "Epoch 3 batch 10: loss = 737.5693969726562\n",
      "Epoch 3 batch 11: loss = 605.9752197265625\n",
      "Epoch 3 batch 12: loss = 702.8925170898438\n",
      "Epoch 3 batch 13: loss = 432.9101257324219\n",
      "Epoch 3 batch 14: loss = 935.0133056640625\n",
      "Epoch 3 batch 15: loss = 363.78936767578125\n",
      "Epoch 3 batch 16: loss = 853.4984741210938\n",
      "Epoch 3 batch 17: loss = 491.4356384277344\n",
      "Epoch 3 batch 18: loss = 773.9022216796875\n",
      "Epoch 3 batch 19: loss = 642.4447021484375\n",
      "Epoch 3 batch 20: loss = 972.908447265625\n",
      "Epoch 3 batch 21: loss = 499.309814453125\n",
      "Epoch 3 batch 22: loss = 643.506103515625\n",
      "Epoch 3 batch 23: loss = 564.5712280273438\n",
      "Epoch 3 batch 24: loss = 442.2332763671875\n",
      "Epoch 3 batch 25: loss = 366.79266357421875\n",
      "Epoch 3 batch 26: loss = 1001.761474609375\n",
      "Epoch 3 batch 27: loss = 800.8330078125\n",
      "Epoch 3 batch 28: loss = 621.2838134765625\n",
      "Epoch 3 batch 29: loss = 536.276123046875\n",
      "Epoch 3 batch 30: loss = 516.718505859375\n",
      "Epoch 3 batch 31: loss = 743.0946655273438\n",
      "Epoch 3 batch 32: loss = 596.0198974609375\n",
      "Epoch 3 batch 33: loss = 513.815185546875\n",
      "Epoch 3 batch 34: loss = 580.443115234375\n",
      "Epoch 3 batch 35: loss = 885.9213256835938\n",
      "Epoch 3 batch 36: loss = 462.1646423339844\n",
      "Epoch 3 batch 37: loss = 764.1090087890625\n",
      "Epoch 3 batch 38: loss = 836.8423461914062\n",
      "Epoch 3 batch 39: loss = 641.364501953125\n",
      "Epoch 3 batch 40: loss = 1067.87646484375\n",
      "Epoch 3 batch 41: loss = 639.5401611328125\n",
      "Epoch 3 batch 42: loss = 734.51123046875\n",
      "Epoch 3 batch 43: loss = 569.3967895507812\n",
      "Epoch 3 batch 44: loss = 774.8538818359375\n",
      "Epoch 3 batch 45: loss = 514.9826049804688\n",
      "Epoch 3 batch 46: loss = 685.5824584960938\n",
      "Epoch 3 batch 47: loss = 564.9667358398438\n",
      "Epoch 3 batch 48: loss = 800.9647827148438\n",
      "Epoch 3 batch 49: loss = 586.7379150390625\n",
      "Epoch 4 batch 0: loss = 485.76617431640625\n",
      "Epoch 4 batch 1: loss = 458.2221984863281\n",
      "Epoch 4 batch 2: loss = 448.7731018066406\n",
      "Epoch 4 batch 3: loss = 943.52734375\n",
      "Epoch 4 batch 4: loss = 565.762451171875\n",
      "Epoch 4 batch 5: loss = 665.5576171875\n",
      "Epoch 4 batch 6: loss = 672.037841796875\n",
      "Epoch 4 batch 7: loss = 765.3156127929688\n",
      "Epoch 4 batch 8: loss = 509.41900634765625\n",
      "Epoch 4 batch 9: loss = 742.4193725585938\n",
      "Epoch 4 batch 10: loss = 423.85565185546875\n",
      "Epoch 4 batch 11: loss = 700.9935913085938\n",
      "Epoch 4 batch 12: loss = 531.791748046875\n",
      "Epoch 4 batch 13: loss = 544.2742309570312\n",
      "Epoch 4 batch 14: loss = 847.1261596679688\n",
      "Epoch 4 batch 15: loss = 605.2373046875\n",
      "Epoch 4 batch 16: loss = 652.822509765625\n",
      "Epoch 4 batch 17: loss = 852.6063232421875\n",
      "Epoch 4 batch 18: loss = 898.5792846679688\n",
      "Epoch 4 batch 19: loss = 684.3765258789062\n",
      "Epoch 4 batch 20: loss = 595.689208984375\n",
      "Epoch 4 batch 21: loss = 780.4011840820312\n",
      "Epoch 4 batch 22: loss = 745.6404418945312\n",
      "Epoch 4 batch 23: loss = 821.27099609375\n",
      "Epoch 4 batch 24: loss = 729.1907958984375\n",
      "Epoch 4 batch 25: loss = 726.6057739257812\n",
      "Epoch 4 batch 26: loss = 653.0880126953125\n",
      "Epoch 4 batch 27: loss = 551.7437744140625\n",
      "Epoch 4 batch 28: loss = 525.1512451171875\n",
      "Epoch 4 batch 29: loss = 748.1895751953125\n",
      "Epoch 4 batch 30: loss = 487.51812744140625\n",
      "Epoch 4 batch 31: loss = 626.6509399414062\n",
      "Epoch 4 batch 32: loss = 609.98486328125\n",
      "Epoch 4 batch 33: loss = 594.4813232421875\n",
      "Epoch 4 batch 34: loss = 422.2345886230469\n",
      "Epoch 4 batch 35: loss = 577.8804321289062\n",
      "Epoch 4 batch 36: loss = 369.119384765625\n",
      "Epoch 4 batch 37: loss = 1091.2344970703125\n",
      "Epoch 4 batch 38: loss = 759.0220336914062\n",
      "Epoch 4 batch 39: loss = 716.1146240234375\n",
      "Epoch 4 batch 40: loss = 878.747314453125\n",
      "Epoch 4 batch 41: loss = 474.2106628417969\n",
      "Epoch 4 batch 42: loss = 398.3009338378906\n",
      "Epoch 4 batch 43: loss = 893.1238403320312\n",
      "Epoch 4 batch 44: loss = 504.133056640625\n",
      "Epoch 4 batch 45: loss = 743.451904296875\n",
      "Epoch 4 batch 46: loss = 731.81494140625\n",
      "Epoch 4 batch 47: loss = 731.53369140625\n",
      "Epoch 4 batch 48: loss = 713.1758422851562\n",
      "Epoch 4 batch 49: loss = 680.8133544921875\n"
     ]
    }
   ],
   "source": [
    "model = LinearNet()\n",
    "loss_fn = paddle.nn.MSELoss()\n",
    "adam = paddle.optimizer.Adam(parameters=model.parameters())\n",
    "\n",
    "dataset = paddle.text.datasets.UCIHousing(mode=\"train\")\n",
    "\n",
    "loader = paddle.io.DataLoader(dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  drop_last=True,\n",
    "  num_workers=2)\n",
    "\n",
    "model.train(loader, loss_fn, adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CstExs3Savqn"
   },
   "outputs": [],
   "source": [
    " test_x = np.array([[-0.0405441 ,  0.06636364, -0.32356227, -0.06916996, -0.03435197,\n",
    "        0.05563625, -0.03475696,  0.02682186, -0.37171335, -0.21419304,\n",
    "       -0.33569506,  0.10143217, -0.21172912]]).astype('float32')\n",
    "\n",
    "df_test_x = pd.DataFrame(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n7qADc_KcJI-"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('test.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(df_test_x.columns)\n",
    "    spamwriter.writerow([-0.0405441 ,  0.06636364, -0.32356227, -0.06916996, -0.03435197,\n",
    "        0.05563625, -0.03475696,  0.02682186, -0.37171335, -0.21419304,\n",
    "       -0.33569506,  0.10143217, -0.21172912])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcrHdbJxAHh0"
   },
   "source": [
    "# **Create BentoService for model serving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_T8YQRjALqg",
    "outputId": "6cad4945-01d4-4d55-dacc-298a971c33ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing paddle_linear_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile paddle_linear_regression.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import bentoml\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.paddle import PaddlePaddleModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([PaddlePaddleModelArtifact('model')])\n",
    "class PaddleLinearRegression(bentoml.BentoService):\n",
    "\n",
    "  @api(input=DataframeInput(), batch=True)\n",
    "  def predict(self, df: pd.DataFrame):\n",
    "        input_data = df.to_numpy().astype('float32')\n",
    "\n",
    "        predictor = self.artifacts.model\n",
    "        input_names = predictor.get_input_names()\n",
    "        input_handle = predictor.get_input_handle(input_names[0])\n",
    "\n",
    "        input_handle.reshape(input_data.shape)\n",
    "        input_handle.copy_from_cpu(input_data)\n",
    "\n",
    "        predictor.run()\n",
    "\n",
    "        output_names = predictor.get_output_names()\n",
    "        output_handle = predictor.get_output_handle(output_names[0])\n",
    "        output_data = output_handle.copy_to_cpu()\n",
    "\n",
    "        return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "ESc4D_muCWNx",
    "outputId": "97885274-8055-4b42-c443-de1e47880078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-04 19:01:27,215] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-04 19:01:28,269 - INFO - Context impl SQLiteImpl.\n",
      "2021-04-04 19:01:28,270 - INFO - Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-04 19:01:28,740] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.0, but loading from BentoML version 0.12.0+15.g7d40998\n",
      "[2021-04-04 19:01:28,779] INFO - BentoService bundle 'PaddleLinearRegression:20210404190128_F4577F' saved to: /Users/bozhaoyu/bentoml/repository/PaddleLinearRegression/20210404190128_F4577F\n"
     ]
    }
   ],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from paddle_linear_regression import PaddleLinearRegression\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "bento_svc = PaddleLinearRegression()\n",
    "bento_svc.pack('model', model)\n",
    "\n",
    "# 3) save your BentoSerivce\n",
    "saved_path = bento_svc.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvUU0k0JCxYk"
   },
   "source": [
    "# **REST API Model Serving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeJEIDyj_xGK",
    "outputId": "0d3d5d3e-d82c-4f8e-d307-78773e49e7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "[2021-04-01 21:29:16,663] INFO - Getting latest version PaddleLinearRegression:20210401212846_F88F15\n",
      "[2021-04-01 21:29:16,683] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-04-01 21:29:16,685] INFO - Starting BentoML API server in development mode..\n",
      "[2021-04-01 21:29:16,890] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-04-01 21:29:16,911] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.0, but loading from BentoML version 0.12.0+14.g0fac537\n",
      "[2021-04-01 21:29:16,914] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      "[2021-04-01 21:29:17,386] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-04-01 21:29:17,405] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.0, but loading from BentoML version 0.12.0+14.g0fac537\n",
      "/usr/local/lib/python3.7/dist-packages/paddle/fluid/backward.py:1640: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  return list(x) if isinstance(x, collections.Sequence) else [x]\n",
      " * Serving Flask app \"PaddleLinearRegression\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      "2021-04-01 21:29:19,971 - INFO -  * Running on http://127.0.0.1:50135/ (Press CTRL+C to quit)\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "C++ Traceback (most recent call last):\n",
      "--------------------------------------\n",
      "0   bvar::detail::SamplerCollector::sampling_thread(void*)\n",
      "1   bvar::detail::SamplerCollector::run()\n",
      "2   paddle::framework::SignalHandle(char const*, int)\n",
      "3   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()\n",
      "\n",
      "----------------------\n",
      "Error Message Summary:\n",
      "----------------------\n",
      "FatalError: `Termination signal` is detected by the operating system.\n",
      "  [TimeInfo: *** Aborted at 1617312560 (unix time) try \"date -d @1617312560\" if you are using GNU date ***]\n",
      "  [SignalInfo: *** SIGTERM (@0x325) received by PID 808 (TID 0x7f191ce54700) from PID 805 ***]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve PaddleLinearRegression:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPoKbR6cCq8_"
   },
   "source": [
    "If you are running this notebook from Google Colab, you can start the dev server with --run-with-ngrok option, to gain acccess to the API endpoint via a public endpoint managed by ngrok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RodE8ooiCqRw",
    "outputId": "7aec7cf2-71ad-4582-93d0-787a7b8b6a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "[2021-04-01 21:30:36,588] INFO - Getting latest version PaddleLinearRegression:20210401212846_F88F15\n",
      "[2021-04-01 21:30:36,609] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-04-01 21:30:36,611] INFO - Starting BentoML API server in development mode..\n",
      "[2021-04-01 21:30:36,731] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-04-01 21:30:36,750] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.0, but loading from BentoML version 0.12.0+14.g0fac537\n",
      "[2021-04-01 21:30:36,751] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      "[2021-04-01 21:30:37,292] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-04-01 21:30:37,309] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.0, but loading from BentoML version 0.12.0+14.g0fac537\n",
      "[2021-04-01 21:30:38,624] INFO -  * Running on http://6a59231a38e1.ngrok.io\n",
      "[2021-04-01 21:30:38,624] INFO -  * Traffic stats available on http://127.0.0.1:4040\n",
      "/usr/local/lib/python3.7/dist-packages/paddle/fluid/backward.py:1640: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  return list(x) if isinstance(x, collections.Sequence) else [x]\n",
      " * Serving Flask app \"PaddleLinearRegression\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      "2021-04-01 21:30:39,718 - INFO -  * Running on http://127.0.0.1:36783/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve PaddleLinearRegression:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMCrkYb5DDHB"
   },
   "source": [
    "# **Make request to the REST server**\n",
    "\n",
    "*After navigating to the location of this notebook, copy and paste the following code to your terminal and run it to make request*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMyLXOIUDXSn"
   },
   "outputs": [],
   "source": [
    "curl -i \\\n",
    "--request POST \\\n",
    "--header \"Content-Type: text/csv\" \\\n",
    "-d @test.csv \\\n",
    "localhost:5000/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RA0JpPjDMt8"
   },
   "source": [
    "# **Containerize model server with Docker**\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that docker is **not available in Google Colab**. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a docker container serving the PaddleLinearRegression prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKUGBMNWDJnr"
   },
   "outputs": [],
   "source": [
    "!bentoml containerize PaddleLinearRegression:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nyRChqMDwv4",
    "outputId": "da95549c-1901-484b-8489-d9b0c9706105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: docker: command not found\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -p 5000:5000 PaddleLinearRegression:20210306050051_766D0A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrnR2hRDD7xf"
   },
   "source": [
    "# **Load Saved Bento Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfxsxBRmD1Tx",
    "outputId": "b3dca212-9359-4bd0-b131-ed766576418d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-06 05:05:25,666] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.7.8, but loading from BentoML version 0.7.8+402.g8c47823\n",
      "[2021-03-06 05:05:25,669] WARNING - Module `paddle_linear_regression` already loaded, using existing imported module.\n",
      "[2021-03-06 05:05:25,680] WARNING - pip package requirement pandas already exist\n",
      "[2021-03-06 05:05:25,681] WARNING - pip package requirement paddlepaddle already exist\n",
      "[[0.85793805]]\n"
     ]
    }
   ],
   "source": [
    "#TODO ADD INPUT\n",
    "from bentoml import load\n",
    "\n",
    "svc = load(saved_path)\n",
    "\n",
    "input = pd.DataFrame([[-0.0405441 ,  0.06636364, -0.32356227, -0.06916996, -0.03435197,\n",
    "        0.05563625, -0.03475696,  0.02682186, -0.37171335, -0.21419304,\n",
    "       -0.33569506,  0.10143217, -0.21172912]]).astype('float32')\n",
    "\n",
    "print(svc.predict(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlGTKeMnEEyE"
   },
   "source": [
    "# **Launch inference job from CLI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fgqIWWIEIva",
    "outputId": "5ad69b69-8165-4d7c-d4b8-360838431edf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "[2021-04-01 21:29:58,275] INFO - Getting latest version PaddleLinearRegression:20210401212846_F88F15\n",
      "[2021-04-01 21:29:58,962] WARNING - Using BentoML not from official PyPI release. In order to find the same version of BentoML when deploying your BentoService, you must set the 'core/bentoml_deploy_version' config to a http/git location of your BentoML fork, e.g.: 'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2021-04-01 21:29:58,978] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.12.0, but loading from BentoML version 0.12.0+14.g0fac537\n",
      "/usr/local/lib/python3.7/dist-packages/paddle/fluid/backward.py:1640: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  return list(x) if isinstance(x, collections.Sequence) else [x]\n",
      "I0401 21:30:01.203775   849 analysis_predictor.cc:155] Profiler is deactivated, and no profiling report will be generated.\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_build_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_clean_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_analysis_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [simplify_with_basic_ops_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [attention_lstm_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [mul_lstm_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_gru_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [mul_gru_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [seq_concat_fc_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [squeeze2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [reshape2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [flatten2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [map_matmul_to_mul_pass]\u001b[0m\n",
      "I0401 21:30:01.213358   849 graph_pattern_detector.cc:101] ---  detected 1 subgraphs\n",
      "\u001b[32m--- Running IR pass [fc_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [repeated_fc_relu_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [squared_mat_sub_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_transpose_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [is_test_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [runtime_context_cache_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_params_sync_among_devices_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [adjust_cudnn_workspace_size_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [inference_op_replace_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [memory_optimize_pass]\u001b[0m\n",
      "I0401 21:30:01.214834   849 memory_optimize_pass.cc:200] Cluster name : linear_1.tmp_2  size: 4\n",
      "I0401 21:30:01.214855   849 memory_optimize_pass.cc:200] Cluster name : linear_1.tmp_1  size: 4\n",
      "I0401 21:30:01.214864   849 memory_optimize_pass.cc:200] Cluster name : x  size: 52\n",
      "I0401 21:30:01.214872   849 memory_optimize_pass.cc:200] Cluster name : x_0  size: 52\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_to_program_pass]\u001b[0m\n",
      "I0401 21:30:01.216336   849 analysis_predictor.cc:598] ======= optimize end =======\n",
      "I0401 21:30:01.216375   849 naive_executor.cc:107] ---  skip [feed], feed -> x\n",
      "I0401 21:30:01.216478   849 naive_executor.cc:107] ---  skip [linear_1.tmp_1], fetch -> fetch\n",
      "2021-04-01 21:30:01.490969: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "[2021-04-01 21:30:04,291] INFO - {'service_name': 'PaddleLinearRegression', 'service_version': '20210401212846_F88F15', 'api': 'predict', 'task': {'data': '0,1,2,3,4,5,6,7,8,9,10,11,12\\r\\n-0.0405441,0.06636364,-0.32356227,-0.06916996,-0.03435197,0.05563625,-0.03475696,0.02682186,-0.37171335,-0.21419304,-0.33569506,0.10143217,-0.21172912\\r\\n', 'task_id': '951521eb-16e6-4dbf-8ff9-6c596a2c9645', 'batch': 1, 'cli_args': ('--format', 'csv', '--input-file', 'test.csv'), 'inference_job_args': {}}, 'result': {'data': '[[0.9648699760437012]]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '951521eb-16e6-4dbf-8ff9-6c596a2c9645'}\n",
      "[[0.9648699760437012]]\n"
     ]
    }
   ],
   "source": [
    "!bentoml run PaddleLinearRegression:latest predict --format csv --input-file test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb-srm9RENeh"
   },
   "source": [
    "# **Deployment Options**\n",
    "\n",
    "If you are at a small team with limited engineering or DevOps resources, try out automated deployment with BentoML CLI, currently supporting AWS Lambda, AWS SageMaker, and Azure Functions:\n",
    "\n",
    "* [AWS Lambda Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
    "* [AWS SageMaker Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
    "* [Azure Functions Deployment Guide](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
    "\n",
    "If the cloud platform you are working with is not on the list above, try out these step-by-step guide on manually deploying BentoML packaged model to cloud platforms:\n",
    "\n",
    "* [AWS ECS Deployment](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
    "* [Google Cloud Run Deployment](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
    "* [Azure container instance Deployment](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
    "* [Heroku Deployment](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
    "\n",
    "Lastly, if you have a DevOps or ML Engineering team who's operating a Kubernetes or OpenShift cluster, use the following guides as references for implementating your deployment strategy:\n",
    "\n",
    "* [Kubernetes Deployment](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
    "* [Knative Deployment](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
    "* [Kubeflow Deployment](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
    "* [KFServing Deployment](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
    "* [Clipper.ai Deployment Guide](https://docs.bentoml.org/en/latest/deployment/clipper.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LinearRegression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
